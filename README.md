#  MUAG - Multi-modal Autonomous Agent (Local)

**MUAG** est un agent autonome multimodal enti√®rement **local**, capable d'interagir vocalement, de r√©fl√©chir et d'ex√©cuter des actions complexes sur votre ordinateur en "voyant" l'√©cran comme un humain.

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Ollama](https://img.shields.io/badge/AI-Ollama_Local-orange)
![OmniParser](https://img.shields.io/badge/Vision-OmniParser_v2-green)
![License](https://img.shields.io/badge/License-MIT-purple)

---

##  D√©monstrations Vid√©o

### 1.  M√©moire & Conversation Contextuelle
*D√©monstration de la persistance de la m√©moire entre les sessions et du rappel des interactions pass√©es.*

> **Exemple :** "Quelle est notre derni√®re interaction ?"
> 
https://github.com/user-attachments/assets/3cbb0a25-2d0e-4800-bcd8-b2f0d3ce6b40

---

### 2.  Actions Rapides (Fast Path)
*Ex√©cution instantan√©e sans planification pour les t√¢ches simples - ouverture d'applications, fichiers, contr√¥le syst√®me.*

> **Exemple :** Ouvrir un fichier texte sur le bureau
>
https://github.com/user-attachments/assets/3015e56e-e181-4c65-86ff-fc4ccf8ed7ae

---

### 3.  Navigation Web & Extraction de Donn√©es
*Pipeline compl√®te : recherche web ‚Üí extraction d'informations ‚Üí cr√©ation de fichier automatique.*

> **Exemple :** Chercher la m√©t√©o et cr√©er un fichier texte avec les r√©sultats sur le bureau
> 
https://github.com/user-attachments/assets/cf3debde-b943-4f05-ad5d-61e2fd17e1a6

---

### 4.  Navigation Visuelle dans Applications Desktop (CUA)
*Contr√¥le autonome d'applications tierces par vision pure - aucune API, juste comme un humain.*

> **Exemple :** Ouvrir Spotify et naviguer dans les titres lik√©s
> 
https://github.com/user-attachments/assets/379c81b7-7fae-44c0-9359-e9f149e7d24e

---

##  Architecture Globale

MUAG repose sur une architecture de d√©cision hybride qui optimise la latence, la fiabilit√© et les capacit√©s autonomes.

```mermaid
graph TD
    User[üé§ Entr√©e Utilisateur<br/>Vocal/Texte] -->|Analyse| IA[üß† Intention Analyzer]
    
    IA -->|Conversation| Conv[üí¨ Conversation<br/>+ M√©moire RAG]
    IA -->|Action Simple| Simple[‚ö° Fast Path<br/>Ex√©cution Directe]
    IA -->|Action Complexe| Orch[üéØ Task Orchestrator]
    
    Conv -->|Contexte| LLM[ü§ñ LLM Local<br/>Ollama]
    LLM -->|R√©ponse| UserOut1[üë§ R√©ponse Utilisateur]
    
    Simple -->|Commandes| Tools[üõ†Ô∏è System Tools]
    Tools --> AppLauncher[üì± AppLauncher]
    Tools --> GUIController[üéÆ GUIController]
    Tools --> FileManager[üìÅ FileManager]
    Tools -->|R√©sultat| UserOut2[üë§ R√©ponse Utilisateur]
    
    Orch -->|Plan d'Action| Skills{üîÄ Skill Router}
    
    Skills -->|Web Simple| WebHelper[üåê WebHelper<br/>Playwright]
    Skills -->|Vision Complex| CUA[üëÅÔ∏è CUA Agent<br/>Computer Use]
    Skills -->|Fichiers| FileManager
    
    WebHelper -->|R√©sultat| Orch
    FileManager -->|R√©sultat| Orch
    
    CUA -->|1. Screenshot| Screen[üì∏ Capture √âcran]
    Screen -->|Image| VLM1[üß† VLM #1 Planificateur<br/>qwen2.5-vl<br/>Analyse + Suggestion]
    
    VLM1 -->|Suggestion| Vision[üîç Vision Pipeline]
    
    Vision -->|2a. UI Detection| Omni[OmniParser<br/>YOLOv8 + Florence-2]
    Vision -->|2b. Text Detection| OCR[PaddleOCR]
    
    Omni --> Enricher[üìä Semantic Enricher<br/>Fusion + Classification]
    OCR --> Enricher
    
    Enricher -->|3. Annotation| Annotator[üé® Visual Annotator<br/>Set-of-Mark]
    
    Annotator -->|Image Annot√©e<br/>+ Suggestion VLM1| VLM2[üéØ VLM #2 Ex√©cuteur<br/>qwen3-vl:4b<br/>Action Pr√©cise]
    
    VLM2 -->|Actions| PyAuto[üñ±Ô∏è PyAutoGUI<br/>Click/Type/Scroll]
    
    PyAuto -->|Feedback| CUA
    CUA -->|Task Complete?| Decision{T√¢che<br/>Termin√©e?}
    Decision -->|Non| Screen
    Decision -->|Oui| Orch
    
    Orch -->|Synth√®se| UserOut3[üë§ R√©ponse Utilisateur]
    
    style User fill:#e1f5ff
    style IA fill:#fff3cd
    style Conv fill:#d4edda
    style Simple fill:#f8d7da
    style Orch fill:#d1ecf1
    style CUA fill:#e2e3e5
    style Vision fill:#f5c6cb
    style VLM1 fill:#c3e6cb
    style VLM2 fill:#bee5eb
    style UserOut1 fill:#e1f5ff
    style UserOut2 fill:#e1f5ff
    style UserOut3 fill:#e1f5ff
```

---

##  Pipeline D√©taill√©e

### 1Ô∏è Analyse d'Intention

Chaque requ√™te utilisateur passe par un **Intention Analyzer** (LLM) qui classifie la demande :

| Type | Description | Route |
|------|-------------|-------|
| **CONVERSATION** | Discussion naturelle, questions g√©n√©rales | ‚Üí Memory Manager + LLM |
| **ACTION_SIMPLE** | Commande directe (1 action) : lancer app, volume, ouvrir fichier | ‚Üí Fast Path (ex√©cution imm√©diate) |
| **ACTION_COMPLEXE** | T√¢che multi-√©tapes : navigation web, interaction app, cr√©ation fichiers | ‚Üí Task Orchestrator |

**Exemples de classification :**
-  "Comment vas-tu ?" ‚Üí CONVERSATION
-  "Lance Spotify" ‚Üí ACTION_SIMPLE
-  "Cherche la m√©t√©o et cr√©e un fichier" ‚Üí ACTION_COMPLEXE

---

### 2 Fast Path (Actions Simples)

Ex√©cution **imm√©diate** sans planification pour minimiser la latence.

**Capacit√©s :**
-  **Lancement d'applications** : d√©tection intelligente par nom (AppLauncher)
-  **Contr√¥le syst√®me** : volume, lecture m√©dia (play/pause/next), captures d'√©cran
-  **Gestion fichiers** : ouverture de fichiers/dossiers/URLs
-  **Raccourcis clavier** : copier/coller, sauvegarder, annuler, etc.

**Mapping intelligent :**
```python
"Monte le volume" ‚Üí volume_up
"Musique suivante" ‚Üí next_track
"Ouvre Chrome" ‚Üí launch_app("chrome")
"Ouvre notes.txt" ‚Üí open_file("Desktop/notes.txt")
```

---

### 3 Task Orchestrator (Actions Complexes)

Le cerveau tactique pour les **t√¢ches multi-√©tapes**.

**Fonctionnement :**

1. **Analyse de la t√¢che** : Le LLM cr√©e un plan initial avec les √©tapes n√©cessaires
2. **Boucle de d√©cision** : √Ä chaque √©tape, d√©cision intelligente du prochain skill √† utiliser
3. **Ex√©cution distribu√©e** : D√©l√©gation aux skills sp√©cialis√©s
4. **Feedback loop** : Mise √† jour du contexte et v√©rification de progression

**Skills disponibles :**

| Skill | Usage | Exemples |
|-------|-------|----------|
| `open_url` | Ouvrir URL initiale | "Va sur YouTube" |
| `web_helper` | Actions web simples (Playwright) | "Clique sur Login", "Cherche m√©t√©o" |
| `cua_vision` | Actions web/desktop complexes (vision) | "Clique sur la 3√®me vid√©o", "Navigue dans Spotify" |
| `file_manager` | Gestion fichiers | "Cr√©e fichier.txt", "Lis document.pdf" |
| `app_launcher` | Lancer applications | "Ouvre Discord" |

**Exemple de plan g√©n√©r√© :**
```
T√¢che : "Va sur YouTube cherche Messi"
‚îú‚îÄ Step 1: open_url ‚Üí https://youtube.com/results?search_query=messi
‚îî‚îÄ Step 2: cua_vision ‚Üí Cliquer premi√®re vid√©o
```

**Protection anti-boucle :** D√©tection automatique de patterns r√©p√©titifs (ABABAB) pour √©viter les blocages infinis.

---

### 4 CUA Agent (Computer Use Agent)

Le c≈ìur de l'innovation : un agent qui **voit** et **agit** comme un humain, sans API.

####  Pipeline Vision Avanc√©e

**Architecture Dual-VLM + OmniParser :**

```
Screenshot ‚Üí Preprocessing ‚Üí Vision Detection ‚Üí Annotation ‚Üí VLM Decision ‚Üí Execution
```

##### **√âtape 1 : Capture & Preprocessing**
- Screenshot de l'√©cran complet
- Preprocessing OpenCV (nettet√©, contraste, d√©bruitage)
- Redimensionnement intelligent (crop 70% droite pour focus zone utile)

##### **√âtape 2 : Vision Detection (Triple Pipeline)**

**2a. OmniParser - UI Element Detection**
- **YOLOv8** : D√©tection rapide des √©l√©ments interactifs (boutons, champs, ic√¥nes)
- **Florence-2** : Compr√©hension s√©mantique fine des widgets et leur fonction

**2b. PaddleOCR - Text Extraction**
- Extraction ultra-pr√©cise de **tout** le texte visible √† l'√©cran
- Support multi-langue, rotation automatique

**2c. Semantic Enricher - Fusion Intelligente**
- Fusionne les r√©sultats OmniParser + OCR
- Classification fonctionnelle (bouton, lien, input, text, etc.)
- Contextualisation spatiale (toolbar, content, footer)
- Description enrichie pour chaque √©l√©ment

**Format de sortie enrichi :**
```json
{
  "id": 42,
  "label": "Connexion",
  "type": "button",
  "functional_type": "primary_action",
  "bbox": [450, 320, 550, 360],
  "center": [500, 340],
  "enriched_description": "Bouton principal 'Connexion' - Action de login",
  "visual_description": "Blue rounded button with white text",
  "ocr_nearby": "Connexion",
  "spatial_context": "content - center-right",
  "confidence": 0.95
}
```

##### **√âtape 3 : Visual Annotation (Set-of-Mark)**
- Reconstruction de l'image avec **bo√Ætes englobantes**
- **ID num√©riques uniques** superpos√©s en vert sur chaque √©l√©ment
- Distinction visuelle par type (boutons en bleu, inputs en rouge, textes OCR en cyan)

##### **√âtape 4 : Dual-VLM Decision**

**VLM #1 - Planificateur (qwen2.5-vl) :**
- Analyse la capture d'√©cran brute
- √âvalue la progression de la t√¢che
- D√©cide si la t√¢che est termin√©e
- Fournit une **suggestion haut niveau** en langage naturel

Exemple :
```json
{
  "description": "Page de r√©sultats YouTube visible",
  "suggestion": "Cliquer sur la premi√®re vid√©o des r√©sultats",
  "task_complete": false
}
```

**VLM #2 - Ex√©cuteur (qwen3-vl:4b) :**
- Re√ßoit l'image **annot√©e** avec IDs visibles
- Re√ßoit la liste d√©taill√©e des √©l√©ments cliquables
- Traduit la suggestion en **action pr√©cise** avec ID d'√©l√©ment

**Optimisation : Cropping Intelligent**
- Le LLM d√©termine la zone pertinente (toolbar/content/footer)
- L'image annot√©e est cropp√©e pour focus maximal
- R√©duction du bruit visuel pour le VLM

Exemple de d√©cision VLM #2 :
```json
{
  "action": "click_on_element",
  "params": {
    "id": 17
  },
  "reasoning": "Cliquer sur l'√©l√©ment 17 (premi√®re vid√©o)"
}
```

**Support des S√©quences :**
```json
{
  "action": "sequence",
  "params": {
    "steps": [
      {"action": "click_on_element", "params": {"id": 8}},
      {"action": "type_text", "params": {"text": "Messi"}},
      {"action": "press_key", "params": {"key": "enter"}}
    ]
  }
}
```

##### **√âtape 5 : Execution (PyAutoGUI)**
- **Correction de scale** : coordonn√©es pr√©process√©es ‚Üí coordonn√©es r√©elles
- Actions pr√©cises : click, type, scroll, hotkeys
- D√©lais adaptatifs pour stabilit√©
- Feedback visuel temps r√©el

**Actions support√©es :**
- `click_on_element` : Clic sur √©l√©ment par ID
- `type_text` : Saisie de texte
- `press_key` : Touche unique (Enter, Tab, Escape)
- `hotkey` : Raccourcis (Ctrl+C, Alt+F4)
- `scroll` : D√©filement
- `wait` : Attente
- `sequence` : Cha√Æne d'actions

##### **√âtape 6 : Feedback Loop**
- Capture du nouvel √©tat apr√®s action
- D√©tection de changements visuels (Screen Monitor)
- Retour √† VLM #1 pour r√©√©valuation
- D√©cision : continuer / changer de strat√©gie / terminer

---

### 5 Playwright Router (Fast-Path Web)

**Optimisation hybride Vision + DOM :**

Avant le pipeline vision lourd, tentative d'ex√©cution **directe via Playwright** :
- Connexion au Chrome en debug mode (CDP)
- Parsing DOM pour matching intelligent
- Scoring multi-strat√©gies (exact, substring, mots-cl√©s)
- Gestion robuste : Shadow DOM, iframes, popups

**Fallback automatique :** Si Playwright √©choue ‚Üí Vision pipeline

**Gain :** Latence divis√©e par 5 sur actions web simples.

---

### 6 WebHelper (Support Playwright Avanc√©)

**Capacit√©s :**
- Connexion CDP avec retry logic
- Scan page avanc√© (clickables + inputs dans Shadow DOM / iframes)
- Matching intelligent avec scoring (exact, substring, fuzzy)
- Actions robustes : click, type, navigate, read
- Gestion automatique des popups/cookies
- Extraction de contenu pour analyse LLM

**D√©tection √©tendue :**
```javascript
// D√©tecte m√™me les inputs cach√©s/iframes
input, textarea, [contenteditable='true'], [role='textbox']
+ iframes + Shadow DOM + data-testid patterns
```

---

##  Fonctionnalit√©s Compl√®tes

###  Interaction Multimodale
- **Full Duplex** : Entr√©e vocale (Whisper) + Sortie vocale (TTS)
- **Mode texte** : Fallback pour debug ou utilisation silencieuse
- **M√©moire persistante** : RAG avec consolidation de session
- **Profil utilisateur** : Apprentissage automatique des pr√©f√©rences (ton, contexte)

###  Contr√¥le Syst√®me (Direct)
- Lancement d'applications intelligent (recherche fuzzy par nom)
- Contr√¥le m√©dia complet (volume, lecture, pause, next/previous)
- Gestion fichiers/dossiers (ouvrir, lire, cr√©er, lister)
- Raccourcis clavier syst√®me
- Captures d'√©cran

###  Navigation Web Autonome
- Recherche Google/YouTube/etc.
- Clic sur √©l√©ments (liens, boutons, menus)
- Remplissage de formulaires
- Extraction de donn√©es
- Lecture de contenu de page
- Gestion automatique des popups/cookies

###  Contr√¥le Desktop Avanc√©
- **Navigation visuelle pure** : Peut piloter **n'importe quelle** application
- Spotify, Discord, VSCode, applications custom
- Pas besoin d'API : utilise uniquement la vision
- D√©tection d'intervention utilisateur (CAPTCHA, login)

###  S√©curit√©s Int√©gr√©es
- Garde-fous pour emp√™cher suppressions accidentelles
- D√©tection de boucles infinies
- Timeouts adaptatifs
- Confirmation utilisateur pour actions sensibles
- **Contr√¥le clavier temps r√©el** : [P] Pause / [C] Continue / [Q] Quit

---

##  Installation

### Pr√©-requis
- **Python 3.10+**
- **Ollama** install√© et en cours d'ex√©cution
- **Google Chrome** (pour WebHelper Playwright)

### Mod√®les Ollama Recommand√©s
```bash
# VLM #1 (Planification) - 4.7GB
ollama pull qwen2.5-vl

# VLM #2 (Ex√©cution) - 2.5GB
ollama pull qwen3-vl:4b

# LLM (Conversation/Orchestration) - 4.7GB
ollama pull qwen2.5
```

### Setup

**1. Cloner le repository**
```bash
git clone https://github.com/SpendinFR/CUAOS.git
cd CUAOS/MUAG
```

**2. Cr√©er environnement virtuel**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

**3. Installer les d√©pendances**
```bash
pip install -r requirements.txt
```

**4. T√©l√©charger les poids OmniParser**
T√©l√©chargez les mod√®les YOLOv8 et Florence-2 dans `weights/` :
```
weights/
‚îú‚îÄ‚îÄ icon_detect/
‚îÇ   ‚îî‚îÄ‚îÄ best.pt  # YOLOv8
‚îî‚îÄ‚îÄ icon_caption_florence/
    ‚îî‚îÄ‚îÄ model.safetensors  # Florence-2
```

**5. Configuration (optionnel)**
Ajustez `config.py` selon vos besoins :
```python
# Mod√®les VLM
TARS_MODEL_NAME = "qwen3-vl:4b"  # Ex√©cution
FALLBACK_VLM_MODEL = "qwen2.5-vl"  # Planification

# Chrome Debug Port (pour Playwright)
CHROME_DEBUG_PORT = 9222

# Contr√¥les
ENABLE_KEYBOARD_CONTROL = True  # Touches P/C/Q
AUTO_CLOSE_POPUPS = True  # Fermer popups web auto
```

**6. Lancer Chrome en mode debug (pour WebHelper)**
```bash
# Windows
"C:\Program Files\Google\Chrome\Application\chrome.exe" --remote-debugging-port=9222

# Linux
google-chrome --remote-debugging-port=9222

# Mac
/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --remote-debugging-port=9222
```

---

##  Utilisation

### Mode Vocal (D√©faut)
```bash
python main.py
```
Parlez naturellement apr√®s le signal sonore.

### Mode Texte (Debug)
```bash
python main.py --text
```
Tapez vos commandes directement.

### Contr√¥les Clavier (pendant ex√©cution CUA)
- **[P]** : Pause - Suspendre l'agent pour observer ou intervenir
- **[C]** : Continue - Reprendre apr√®s pause
- **[Q]** : Quit - Arr√™t propre de l'agent

---

##  Exemples de Commandes

###  Conversation
```
"Comment vas-tu ?"
"Quelle est notre derni√®re interaction ?"
"Quel est mon film pr√©f√©r√© ?" (si m√©moris√©)
```

###  Actions Simples
```
"Lance Spotify"
"Monte le volume"
"Ouvre Chrome"
"Ouvre le dossier Documents"
"Ouvre notes.txt sur le bureau"
"Musique suivante"
"Fais une capture d'√©cran"
```

###  Navigation Web
```
"Va sur YouTube et cherche Messi"
"Cherche la m√©t√©o √† Paris sur Google"
"Va sur Gmail et lis mon dernier email"
"Ouvre Spotify web et lance ma playlist"
```

###  Gestion Fichiers
```
"Cr√©e un fichier meteo.txt sur le bureau avec la temp√©rature actuelle"
"Lis le fichier notes.txt"
"Liste les fichiers du dossier Documents"
```

###  T√¢ches Complexes
```
"Va sur Google, cherche 'prix Bitcoin', copie le r√©sultat et cr√©e un fichier crypto.txt"
"Ouvre Spotify et joue ma derni√®re playlist"
"Va sur YouTube, cherche un tutoriel Python et ouvre la premi√®re vid√©o"
```

---

##  Technologies Utilis√©es

| Composant | Technologie | R√¥le |
|-----------|-------------|------|
| **LLM** | Ollama (Qwen2.5) | Conversation, planification, d√©cisions |
| **VLM Planif** | Qwen2.5-VL (7B) | Analyse √©cran, v√©rification t√¢che |
| **VLM Exec** | Qwen3-VL (4B) | Ex√©cution pr√©cise avec visual grounding |
| **UI Detection** | YOLOv8 | D√©tection rapide √©l√©ments interactifs |
| **Semantic Vision** | Florence-2 | Compr√©hension s√©mantique UI |
| **OCR** | PaddleOCR | Extraction texte haute pr√©cision |
| **Web Automation** | Playwright | Actions web DOM-based (fast-path) |
| **Desktop Control** | PyAutoGUI | Ex√©cution actions physiques (click/type) |
| **Vision Processing** | OpenCV | Preprocessing, annotation visuelle |
| **Speech-to-Text** | Whisper | Reconnaissance vocale |
| **Text-to-Speech** | Coqui TTS | Synth√®se vocale locale |

---

##  Architecture Technique

**Modularit√© :**
- `brain/` : Analyseur d'intention, orchestrateur, ex√©cuteur
- `actions/` : Skills sp√©cialis√©s (CUA, WebHelper, FileManager, etc.)
- `utils/` : Clients LLM/VLM, m√©moire, profil utilisateur
- `config.py` : Configuration centralis√©e

**Points forts :**
-  **100% Local** : Aucun appel cloud, donn√©es priv√©es
-  **Hybride** : Vision + DOM pour performance optimale
-  **Adaptable** : Fonctionne sur n'importe quelle interface
-  **Robuste** : Fallbacks multiples, d√©tection erreurs
-  **Extensible** : Ajout facile de nouveaux skills

---

##  Troubleshooting

**Erreur : "WebHelper not connected"**
- V√©rifiez que Chrome est lanc√© avec `--remote-debugging-port=9222`
- Port d√©j√† utilis√© ? Changez dans `config.py`

**VLM timeout**
- R√©duisez la r√©solution d'image dans `config.py`
- Mod√®les trop lourds ? Essayez `qwen3-vl:4b` uniquement

**Actions impr√©cises**
- Ajustez `scale_factor` si multi-√©crans
- Activez `ENABLE_PREPROCESSING` dans `config.py`

**Boucle infinie d√©tect√©e**
- L'agent s'arr√™te automatiquement apr√®s 6 r√©p√©titions
- V√©rifiez les logs pour identifier l'action bloqu√©e

---

##  Contribution

Les contributions sont les bienvenues ! Points d'am√©lioration :
- Support multi-langues (actuellement fran√ßais)
- Nouveaux skills (email, calendrier, etc.)
- Optimisation VLM (quantization, distillation)
- Tests automatis√©s

---

##  Licence

MIT License - Voir `LICENSE` pour d√©tails

---

##  Remerciements

- **Microsoft** : OmniParser (YOLOv8 + Florence-2)
- **PaddlePaddle** : PaddleOCR
- **Anthropic** : Inspiration architecture Computer Use
- **Ollama** : Inf√©rence locale simple
- **Alibaba Cloud** : Mod√®les Qwen

---

**‚ö†Ô∏è Disclaimer :** MUAG est un projet exp√©rimental. Utilisez-le de mani√®re responsable et s√©curis√©e. L'agent peut effectuer des actions syst√®me - assurez-vous de comprendre ce qu'il fait avant de l'autoriser sur des environnements de production.

---

*Projet d√©velopp√© pour repousser les limites des agents autonomes locaux* üöÄ
